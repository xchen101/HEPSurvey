# Freetext comments

## When do you think is it appropriate to openly share research data and code? (TH)
1. I think data and code should be treated separately. Research data: as soon as it is submitted to arXiv; Code: probably some time after the publication of the paper2. The big question is: what to share. Calibrated tracks of filtered events that lead to the given result? Or the points that are on the published figures? It is a big difference!
3. Code for public should be usable for everybody not only the authors

## When do you think is it appropriate to openly share research data and code? (EX)
1. Code should be shared immediately and always, with care taken not to leak results. Data can wait until the collaborators have had a chance to exploit it first.2. See answer to next question.  Totally open sharing is not appropriate.
3. I think that releasing data should be At the same time as the publication of the paper, but I don’t agree on sharing the code
4. I’d share data, not the code. Sharing after publication.
5. Question is too simple minded
6. in any relatively complex research, releasing data and code in public domain doesn’t change anything, basically nobody will have the knowledge to use those data and code
7. after an embargo period set by collaborations
8. Code needs to be examined both for internal review and publication. Result data publication should accompany the publication. Input datasets, like a CMS dataset, should be published separately following the practices of telescope data.
9. Large collaborations have datasets used by 100s of analyses. When the raw dataset should be released needs careful consideration.
10. From the beginning
11. A given time after the publication
12. After the experiment has made the most of the collected data, i.e., some years after its collection
13. If there is no reservations, such as potential commercial use
14. At least one year after publication.
15. When the dataset has been analyzed by the collaboration, that is, typically after 3-5 years.
16. When the data are in a form that is easily comprehensible and small enough in scale. Releasing petabytes of raw data is utterly useless and extremely expensive.
17. 17 In order to release data, you need to describe openly all calibration, source code etc. This is a lot a lot of work., which is absolutely not comparable to the publication workload. I have done this once. The dataset was published and documented 3 years later.
18. Research data can be openly released by the time as paper is published, the analysis code should be released only within collaboration. For full disclosure of code I think the best time is a few years after particular collaboration stops be active.
19. Since the same data is processed in many analyses, it takes time before the experiments have confidence in sharing the data20. It is appropriate to share data, it is not always appropriate to share code

## In your opinion who should decide the accessibility of research data and code?
1. Funding bodies should decide, but moreover should provide the means to maintain that, not just policies to be followed. Without that a collaboration-level decision is what is more appropriate.
2. The HEP community as a whole should decide and set practices for data. Code publication should be decided by code authors, but should be required to be reviewed with rules set by collaborations or the field as a whole.
3. i don’t see any of the above clear good solutions
4. No one should decide. If limits are set to data accessibility, it will no longer be open. Anybody should be able to access with no restrictions.
5. bureaucrats should mind their own buisness and leave us alone to do real work
6. I think models like the one astronomy uses where data collection and data analysis are factorized are good models. Anyone who pays for access should then be able to analyze data, and the release of analysis data and code should be part of the peer review process. How that data/code is released could vary by where it is published. Since most of the data collection is publicly funded, the raw data is owned by the public, not the collaboration; so there would be input from them as well.
7. It depends (abuse, funding, owner, …)

## Reason why choosing “I have no interest in sharing.”
1. I put much effort into my research so why should I let someone that does nothing have access to my work?
2. Near impossibility to explain all subtleties of my code and data. Fear of missinterpretation - unintentional or intentional -  of results.
3. If we speak on the input data to analysis, I think it will be misused and results most likely incorrect. Then it will fall back on us to disprove the results.
4. Raw data - which is what I assume you are talimng about, is unintelligible to other collaborations
5. Why would people take data, if they can get it for free.
6. Complexity of data and its analysis from my experiments requires scrutiny of fellow collaborators. This kind of scrutiny cannot be provide in publishing stage. I know of multiple examples of worthless work published when data was analyzed without such controls. Irresponsible data sharing can mean death of science. I am not against data/code sharing in general. However, one model fits all in dangerous.
7. Would require a lot of additional work, no incentive to do it from metrics used by future employers.
8. I don’t think very complex analysis software is useful.
9. just has never come up, in my experience
10. Collaborations share data and code internally. There is no benefit to sharing code or data externally.
11. We usually need the results not the raw data and the code as one needs a lot of internal knowledge to be able to repeat the full data extraction
12. I do not use codes and/or data for my research. My research is purely formal
13. Data are too complex to be analysed by outsiders

## Comments about providing access to different types of research artifacts for strengthening peer review process
1. If we are being honest, the only justification against open data is selfish advantage. For the progress of the field, researchers should always default to open data unless there are other extenuating factors. Just look at the revolution brought about by open source software!
2. ~~**_Unfortunately, twiki, slides, analysis notes and even papers are very often not up to date or incomplete in describing what has been done. _**For exactly that reason you need the code and data to understand what is happening..~~
3. ~~**_I believe that the focus of peer-review should be maintained on the physics and not on the ancillary tools used for it. _**Code is purely and instrument, and I think that sharing it and the corresponding data is motivated only if it can ease the access of people to the content of a research. Instead of making efforts in documenting the workflow of an analysis, I think it would be more interesting to ensure that the description, the methods and the results of the research are clearly documented and correctly explained in a paper.~~
4. It is really up to the collaboration and referees to decide.
5. passing down scripted workflows creates a real blackbox lack of understanding. click & collect should not be encouraged.
6. Only those who produce the code and data know all subtle details of the analysis. Independent referee may be more confused than enlightened. **_Published results must be reproducible by independent research not biased by existing code and data._**
7. ~~Not clear if you mean internal or external peer review~~
8. ~~**_Reviewers have barely time to review the paper. _**They simply will not review any code or complicated content.~~
9. ~~I like Open/Libre most things.~~
10. ~~I doubt the reviewers will be able to go over data or code…~~
11. ~~I think it would be very helpful to the review process if guidelines existed in how to make an analysis reproducible using a standard process and standard software. That should include material on how to fulfill such guidelines in practice.~~
12. ~~I assumed when filling this survey that the data we are atlking about is the input to the analysis, not the final tables/figures that we already share with e.g. HepData.~~
13. ~~**_Large HEP collaborations have alreay adopted twiki pages and internal notes as standard components for the internal review process. _**This may become an overhead difficult to stand for small collaborations.~~
14. ~~I think that the idea of public code is interesting, but evaluating it as part of the peer-review procedure is not feasible.  **_Furthermore, such practices would have to be globally enforced, or else some experiments will have much faster publications than others due to the amount of time it takes to make something externally usable without understanding all of the details.  _**For the large LHC collaborations, there are literally thousands of people working on the code for results, and it often takes very specialized people to connect everything together.  **_Releasing it publicly in a form that anyone can pick up and use is an enormous overhead that would likely block physics results._**~~
15. ~~**_Full Data & code are too complex in HEP to be reviewable per paper._** The experiments internal review before publication already are meant to take care of that. Wiki pages and presentations could be helpful, but might be more confusing for an external audience that is not following the experiment’s history. My impression is that **_if all code/data went into the review, publishing would become extremely time consuming for everybody._**~~
16. ~~I find this survey very difficult. For example,  questions which mix data and code can have very different answers~~
17. ~~**_data or code would be unhelpful, but both data and code would be very helpful_**~~
18. ~~I assumed that “peer review process” means external-to-the-collaboration journal-level peer review process. The answers would have been quite different if by “peer review” the internal peer review within the same experimental collaboration was meant.  **_Complexity of most of contemporary analyses from high-energy experiments is such that even internal review, done by people familiar with the equipment, data –taking conditions and software infrastructure is hard to do. An idea that this could be outsourced to external journal reviewers is a dangerous lunacy._**~~
19. ~~There is an ambiguity of data/code at which level is to be provided. **_In HEP I am typically not interested in the analysis code within some experiment software which I can’t run anyway, but on an analysis code which runs in e.g. Rivet or other frameworks for analysis preservation after detector corrections._**~~
20. ~~**_Although note that even within a collaboration, with documentation for new collaborators, it’s really hard to get up to speed on the software.  I doubt reviewers will want to put in weeks learning how to use these proposed tools._**~~
21. ~~**_it’s not useful to distringuish between code and script._** in general by code one should consider any computer program.or system used in an analysis (including DB data, resource files)~~
22. Most physics papers are just text descriptions of their code. The code is the publication and should be the primary focus of peer review; not the paper.
23. ~~I do data management for HEP.  **_Open access to full data is technologically impossible without more resources to support it even though I’d like it to be accessible._**~~
24. I feel this might make a bigger difference in smaller, non-HEP experiments like some of the dark matter experiments or smaller astronomical surveys.
25. ~~**_Public release of data and code from a large experiment seems to be completely unrealistic and of little use to anyone._**~~
26. ~~don’t know how much time peer reviewers have~~
27. ~~The software is often very complex, who outside of the collaboration would have the time and will to actually review it?~~
28. ~~will the reviewer have time to try to look at this?~~
29. ~~Making the huge and complex HEP data sets _and_ code available would be too complicated.~~
30. ~~**_Scripted workflow is part of the code._**~~
31. ~~External review of INTERNAL collaboration documentation (code, data, pages, slides, scripts) is a huge amount of effort for the peer reviewers.  The internal collaboration review process has a lengthy number of steps with expert review inside the collaboration.~~
32. ~~**_The real issue is the complexity and size of the underlying code and data_**~~
33. ~~The main problem with open data, is that other researchers will use your code to reproduce your results only on rare occasions, most of the time they will use it for their own benefits, to quickly publishing papers. You do the hard work, they get the credit …~~
34. ~~I assume “data” means analysis files, not raw data.  Also, what we call “data” most vary greatly between experiments.~~
35. ~~**_Reviewing the code etc will be much too difficult/time consuming for an external reviewer._** However, the internal process to review code and verifications of its correctness should be documented alongside the manuscript.~~
36. ~~dumping millions of lines of code on a (physics?) reviewer (who might speak a different programming language) won’t get us anywhere. **_“analysis code” is way too vaguely defined and any substantial amount is unreviewable._**~~
37. ~~**_large experimental collaboration analysis code often has dependencies _**on other parts of the code and would be impossible to review in a timely fashion. However, there are internal reviews and plenty of validation plot cross-checks that can and are done. So one has to be careful what is deemed a “code review”.~~
38. Elements that are likely not to be accessed by future readers of the paper should not be considered in the review process
39. ~~Increasing the burdens on reviewers, who are unpaid, is a bad direction to go and should be discouraged.~~
40. ~~**_Although the code might be interesting, it will slow down the process enormously and in most cases it just does not help_**~~
41. ~~The collaboration needs to have a **_safe space_** where statements can be exchanged that are not readily defendable in front of the general public. For this reason, e.g. slides, can only contain material approved by the collaboration and hence reproduce the paper content + the extra material.~~
42. ~~reviewing this will be such a burden on the peer reviewer, they will never go through it~~
43. ~~**The basic problem in sharing open data, and the reason why I am reluctant to support it completely, that data as such without a lot of auxiliary knowledge present in the experiment, could be misinterpreted and lead to wrong results**~~
44. Can someone finally fix vydio to be usable with Ubuntu?!?!
45. ~~**Hard to separate out general analysis framework of a collaboration from specific codebase of one paper. **A 50k+ line code base can not feasibly be reviewed, snippets can but then there is a strong selection bias.~~
46. ~~There is no need to have coding and data as part of the review. Once they are public anyone else could publish papers that confirm or not the paper.~~
47. ~~Code snippets and summaries are great (i.e. what we post on HEPData in Rivet format, etc.). Open data is less necessary IMO— **too many caveats on the analysis to make the data useful to the public.**~~
48. Presentation Slides can be sometime misleading: Researchers learn from discussions. Unless, very carefully documented, presented slides in internal meetings/gathering can be totally misleading sometimes. However, conference slides should be OA.
49. ~~As peer reviewers we are volunteers with extremely little time.  We would need an entirely new model of peer review if the task of checking data and code were added to review responsibilities.  Furthermore, most data and code are extremely difficult to understand in HEP if you don’t already know details of the detector.~~
50. For my field (theory), open access is the most important. An issue not discussed in the survey is the cost of the journal subscriptions — not open access — for cash-strapped universities and research centres.
51. ~~difficult to answer in such general sense~~
52. ~~Peer review is long enough, adding code would likely just slow things down~~
53. ~~sharing this kind of information with a referee is pointless, since they are not going to re-run. Our internal review process has already done this, and there is no point in duplicating effort~~
54. ~~One difficulty with HEP data is that it usually corresponds to i) very large data sample ii) analysed with complex software suite, making it difficult to release fully. While it would be a sane option for the future (re-analyse LHC data back in 2050 would be great), **_data preservation and code analysis maintenance is a tedious and ressource greedy task._**~~
55. ~~Here I see a difference between publication peer review and the collaborartion peer review. Everything should be available within the collaboration and the responses above are about the publication peer reviewers.~~
56. ~~**_I would consider all, any, or none of these on a case-by-case basis_**~~
57. The gain is on both sides: analyser and reviewer. Providing those inputs as part of the review process ensures a higher quality in the data analysis practices.
58. ~~**It takes a lot of time for the analyzers to make their codes, it will take a lot of time for anyone else to understand them as well. **Therefore it’s not really “easy” to share in my opinion. It’s not as simple as providing images to spot aliens.~~
59. ~~**The complexity of a HEP analysis is such that only people that worked on it can correctly analyse it.**~~
60. ~~Too generic. Which Data, which Code should be accessible in addition to manuscript ?~~
61. ~~attention should be paid not to overload referees…~~
62. ~~peer review already is a burden, more so for the reviewers than for the authors~~
63. ~~How much addintional work will be created?~~
